# -*- coding: utf-8 -*-
"""Selegor_Script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SlDm2nOZEYlHEp4M9Q3qUidg9Gl-RTAA
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import random
import time
from fake_useragent import UserAgent
import re

# Define correct base URL for all pages
BASE_URL = "https://www.seloger-construire.com"

# Listing page URL
LISTING_URL = f"{BASE_URL}/projet-construction/maison-terrain/pays/france"

# Proxy list (Rotating Proxies)
PROXY_LIST = [
    "http://8598bdc273a3fa603e70:ea0531b98630b3e6@gw.dataimpulse.com:823",
    "http://8598bdc273a3fa603e70:ea0531b98630b3e6@gw.dataimpulse.com:823",
    "http://8598bdc273a3fa603e70:ea0531b98630b3e6@gw.dataimpulse.com:823"
]

# Function to get a random User-Agent
def get_headers():
    ua = UserAgent()
    return {
        "User-Agent": ua.random,
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": "https://www.google.com/",
        "Connection": "keep-alive"
    }

# Function to get a random proxy
def get_proxy():
    return {"http": random.choice(PROXY_LIST), "https": random.choice(PROXY_LIST)}

# Function to extract `property_id` from detail page URL
def extract_property_id(detail_url):
    match = re.search(r'/(\d+)/$', detail_url)
    return match.group(1) if match else "N/A"

# Function to scrape property details from a detail page
def scrape_detail_page(detail_url):
    headers = get_headers()
    proxy = get_proxy()

    print(f"   ‚ûú Scraping Detail Page: {detail_url}")

    try:
        response = requests.get(detail_url, headers=headers, proxies=proxy, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # Extract Property ID
        property_id = extract_property_id(detail_url)

        # Extract phone number
        phone_number_tag = soup.find('a', class_='phoneBtnInlineNumberLink')
        phone_number = phone_number_tag['href'].replace('tel:', '').strip() if phone_number_tag else "N/A"

        # Extract image URL
        image_tag = soup.find('li', class_='detailAnnonceCarouselItem')
        image_url = image_tag['data-src'] if image_tag and 'data-src' in image_tag.attrs else "N/A"

        # Extract description
        description_div = soup.find('div', class_='detailAnnonceDescriptionContent')
        description = description_div.get_text(strip=True) if description_div else "N/A"

        # Extract address
        location_tag = soup.find('span', class_='detailAnnonceInfosCity')
        address = location_tag.get_text(strip=True) if location_tag else "N/A"

        # Extract price
        price_tag = soup.find('p', class_='detailAnnonceInfosPrice')
        extracted_price = "N/A"
        if price_tag:
            price_text = price_tag.get_text(strip=True)
            match = re.search(r'[\d\s]+‚Ç¨', price_text)  # Extracts numbers followed by ‚Ç¨
            extracted_price = match.group() if match else "N/A"

        return {
            "Property ID": property_id,
            "Phone Number": phone_number,
            "Image URL": image_url,
            "Description": description,
            "Address": address,
            "Price (‚Ç¨)": extracted_price,
            "Website Name": "Seloger",
            "Expired": False  # Leads are fresh initially
        }

    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error scraping detail page {detail_url}: {e}")
        return None

# Function to scrape all listing pages and extract property details (Stops at 600 leads)
def scrape_all_pages():
    all_properties = []
    unique_property_ids = set()  # Track unique properties
    current_url = LISTING_URL
    page_number = 1
    max_retries = 3  # Number of retries before skipping a page

    while current_url:
        headers = get_headers()
        proxy = get_proxy()

        print(f"\nüü¢ Scraping Listing Page {page_number}: {current_url}")

        retry_attempts = 0  # Track retry attempts for a page

        while retry_attempts < max_retries:
            try:
                response = requests.get(current_url, headers=headers, proxies=proxy, timeout=15)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, "html.parser")

                # Extract all property detail page links
                listings = soup.find_all('a', class_='AnnounceCard_announceCardSlider__CaJaL')
                for listing in listings:
                    detail_link = listing.get('href')
                    if detail_link:
                        detail_url = f"{BASE_URL}{detail_link}"  # Construct full detail page URL
                        property_id = extract_property_id(detail_url)

                        # Check if this property is already added
                        if property_id not in unique_property_ids:
                            property_details = scrape_detail_page(detail_url)  # Scrape full details
                            if property_details:
                                all_properties.append(property_details)  # Store data
                                unique_property_ids.add(property_id)

                        # Stop when 600 unique properties are collected
                        if len(unique_property_ids) >= 10:
                            print("\n‚úÖ 600 Properties Scraped. Stopping Scraper.")
                            return pd.DataFrame(all_properties)

                    time.sleep(random.uniform(2, 5))  # Delay to avoid blocking

                # Find the next page link
                next_page_link = soup.find('a', {'data-testid': 'gsl.uilib.Paging.nextButton'})
                if next_page_link and 'href' in next_page_link.attrs:
                    current_url = f"{BASE_URL}{next_page_link['href'].strip()}"
                    page_number += 1
                else:
                    print("\n‚úÖ No more pages found. Scraping complete.")
                    return pd.DataFrame(all_properties)  # Stop when there are no more pages

                break  # Exit retry loop if successful

            except requests.exceptions.RequestException as e:
                retry_attempts += 1
                print(f"‚ùå Error scraping listing page {current_url} (Attempt {retry_attempts}/{max_retries}): {e}")

        # If max retries reached, skip the page
        if retry_attempts == max_retries:
            print(f"‚ö†Ô∏è Skipping Listing Page {page_number} after {max_retries} failed attempts.")
            page_number += 1
            current_url = f"{BASE_URL}/projet-construction/maison-terrain/pays/france?page={page_number}"

    return pd.DataFrame(all_properties)  # Return collected data


def preprocess_data(df):
    df["Price (‚Ç¨)"] = df["Price (‚Ç¨)"].str.replace("‚Ç¨", "").str.replace(" ", "").str.strip()
    df["Phone Number"] = df["Phone Number"].str.replace(r"\s+", " ", regex=True).str.strip()
    df["Address"] = df["Address"].str.replace(r"\s{2,}", " ", regex=True).str.strip()
    df["Property Name"] = df["Address"]
    df["Status"] = "FRESH"
    desired_order = ['Property Name', 'Price (‚Ç¨)', 'Description', 'Address', 'Phone Number', 'Image URL', 'Website Name', 'Expired', 'Status']
    df = df[desired_order]

    print("\n‚úÖ Data preprocessing completed successfully!")
    return df
# Run scraping and preprocessing
def run_scraper():
    df = scrape_all_pages()
    df_cleaned = preprocess_data(df)

    df_cleaned.to_csv("Seloger_Properties.csv", index=False)
    # df_cleaned.to_json("Cleaned_Seloger_Properties.json", orient="records")

    print("\n‚úÖ Scraping & Preprocessing Completed Successfully!")
    return df_cleaned


if __name__ == "__main__":
    run_scraper()